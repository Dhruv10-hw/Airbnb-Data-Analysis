# -*- coding: utf-8 -*-
"""CIS9650-Group5-airbnb(a).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ltx06M2OqayYCeg2O2qsN0lJpOZElZaO
"""

# CIS 9650 Term Project
# Group 5
# AirBnB Data Analysis
# Team Member: Krishi Shah, Devansh Bhavsar, Nisargvan Goswami, Dhruv Sharma, Devarshi Lala

## Part 1: Import data using Kaggle API for data extraction and loading

!mkdir -p ~/.kaggle
!cp /content/kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

import os
os.environ['KAGGLE_USERNAME'] = 'dhruvsharma10'
os.environ['KAGGLE_KEY'] = '87d3cb9ea26bc2e70ce34bc3e215c1b1'

from kaggle.api.kaggle_api_extended import KaggleApi
import pandas as pd
import numpy as np
import os

#Initialize and authenticate the Kaggle API
api = KaggleApi()
api.authenticate()

#Define the dataset identifier and the specific file name based on the website
dataset_identifier = "dgomonov/new-york-city-airbnb-open-data"
filename = "AB_NYC_2019.csv"

#Download the specific file directly, specifying the download path
api.dataset_download_file(dataset_identifier, filename, path=os.getcwd())

#Construct the full file path using os.path.join
file_path = os.path.join(os.getcwd(), filename)

#Load the downloaded file into a Pandas DataFrame using the constructed path
data = pd.read_csv(file_path)

#Display the first few rows of the DataFrame
print(data.head())

#By utilizing Kaggle API, our group successfully got the dataset in, and the data structure looks clean and organized

## PART II: Clean Up the Dataset
## Get a summary of the dataset and check if the column names are meaningful
print("Basic Information:")
data.info()

## Check Null Record and Remove Null Record in the dataframe
null_counts = data.isnull().sum()
print(null_counts)

##It shows that name, Host_name, last_review and review_per_month has missing value other then that the data is clean.

"""**Cleaning the data To convert missing values to 0 or unknown. And Removing last review as a whole as it is nt relevant to our research purpose.**"""

# 1. Handle 'host_name' (21 missing values)
# 2. Handle 'name' (16 values missing)
#3. Handle 'review_per_month' (10052 values missing)
data[['host_name', 'name', 'reviews_per_month']] = data[['host_name', 'name', 'reviews_per_month']].fillna({'host_name':'Unknown', 'name':'Unknown', 'reviews_per_month':0})

null_counts = data[['host_name', 'name', 'reviews_per_month']].isnull().sum()
print(null_counts)

import pandas as pd

# Assuming `data` is your DataFrame
total_records = data['last_review'].count()
print(f"Total records for 'last_review': {total_records}")

# Remove specified columns in-place
columns_to_remove = ['last_review', 'name', 'id', 'host_name']
data.drop(columns=columns_to_remove, axis=1, inplace=True)

# Display the first few rows to confirm removal
print(data.head())

## Feedback:last_review, name. id, host_name were removed successfully for cleaning purpose

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Define the columns to analyze
columns_to_analyze = ['price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']

# Function to analyze columns
def analyze_column(df, column):
    print(f"Analysis of '{column}':")

    # Calculate statistics
    mean_val = df[column].mean()
    median_val = df[column].median()
    std_dev = df[column].std()
    min_val = df[column].min()
    max_val = df[column].max()
    col_range = max_val - min_val
    percentiles = df[column].quantile([0.25, 0.5, 0.75, 0.9, 0.99])
    count = df[column].count()

    # Outlier Detection using IQR
    q1 = percentiles[0.25]
    q3 = percentiles[0.75]
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

    # Print results
    print(f"  Count: {count}")
    print(f"  Mean: {mean_val:.2f}")
    print(f"  Median: {median_val:.2f}")
    print(f"  Standard Deviation: {std_dev:.2f}")
    print(f"  Range: {col_range:.2f} (Min: {min_val:.2f}, Max: {max_val:.2f})")
    print("  Percentiles:")
    print(percentiles)
    print(f"  Outliers: {len(outliers)} (Lower Bound: {lower_bound:.2f}, Upper Bound: {upper_bound:.2f})")
    print("\n")

    # Visualization: Distribution
    plt.figure(figsize=(8, 4))
    sns.histplot(df[column], bins=30, kde=True, color='blue', edgecolor='black')
    plt.axvline(mean_val, color='red', linestyle='--', label='Mean')
    plt.axvline(median_val, color='green', linestyle='-', label='Median')
    plt.title(f"Distribution of {column}")
    plt.xlabel(column)
    plt.ylabel("Frequency")
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

# Analyze each column
for column in columns_to_analyze:
    analyze_column(data, column)

# Calculate percentage of listings by room_type
room_type_counts = data['room_type'].value_counts()
total_listings = room_type_counts.sum()
room_type_percentages = (room_type_counts / total_listings) * 100

print("Percentage of listings by room_type:")
print(room_type_percentages)
print("\n")

# Calculate percentage of listings by neighbourhood_group
neighbourhood_group_counts = data['neighbourhood_group'].value_counts()
neighbourhood_group_percentages = (neighbourhood_group_counts / total_listings) * 100

print("Percentage of listings by neighbourhood_group:")
print(neighbourhood_group_percentages)

## Create a new data frame to store the key statistc calculated based on prior codes
import pandas as pd

# Load the data
data = pd.read_csv('AB_NYC_2019.csv')

# Analyze the 'price' column
price_column = 'price'

# Calculate basic statistics
count = data[price_column].count()
mean_val = data[price_column].mean()
std_dev = data[price_column].std()
median_val = data[price_column].median()
min_val = data[price_column].min()
max_val = data[price_column].max()
col_range = max_val - min_val
percentiles = {
    "25th Percentile": data[price_column].quantile(0.25),
    "50th Percentile (Median)": data[price_column].quantile(0.5),
    "75th Percentile": data[price_column].quantile(0.75),
    "90th Percentile": data[price_column].quantile(0.9),
    "99th Percentile": data[price_column].quantile(0.99)
}

# Calculate outliers using IQR
q1 = percentiles["25th Percentile"]
q3 = percentiles["75th Percentile"]
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
outliers = data[(data[price_column] < lower_bound) | (data[price_column] > upper_bound)].shape[0]

# Compile all results into a dictionary
price_analysis = {
    "Column": price_column,
    "Count": count,
    "Mean": mean_val,
    "Standard Deviation": std_dev,
    "Median": median_val,
    "Range": col_range,
    "Min": min_val,
    "Max": max_val,
    "25th Percentile": percentiles["25th Percentile"],
    "50th Percentile (Median)": percentiles["50th Percentile (Median)"],
    "75th Percentile": percentiles["75th Percentile"],
    "90th Percentile": percentiles["90th Percentile"],
    "99th Percentile": percentiles["99th Percentile"],
    "Lower Outlier Bound": lower_bound,
    "Upper Outlier Bound": upper_bound,
    "Number of Outliers": outliers
}

# Create a DataFrame
analysis_df = pd.DataFrame([price_analysis])

# Display the DataFrame
print("Analysis of Price")
print(analysis_df.T)

# Plot the distribution
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 4))
sns.histplot(data[price_column], bins=30, kde=True, color='blue', edgecolor='black')
plt.axvline(mean_val, color='red', linestyle='--', label='Mean')
plt.axvline(median_val, color='green', linestyle='-', label='Median')
plt.title(f"Distribution of {price_column}")
plt.xlabel(price_column)
plt.ylabel("Frequency")
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Load the data
data = pd.read_csv('AB_NYC_2019.csv')

# Calculate mean price for each room type
room_type_avg_price = data.groupby('room_type')['price'].mean().sort_values(ascending=False)

# Define colors for the bars (one color per room type)
colors = ['skyblue', 'salmon', 'lightgreen']

# Plot the bar chart
plt.figure(figsize=(10, 6))
bars = plt.bar(room_type_avg_price.index, room_type_avg_price.values, color=colors)

# Add annotations to each bar
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height, f'${height:.2f}', ha='center', va='bottom')

# Customize the plot
plt.xlabel('Room Type')
plt.ylabel('Average Price')
plt.title('Average Price by Room Type')
plt.xticks(rotation=45)
plt.tight_layout()

# Display the chart
plt.show()

# Calculate average price for each room type and neighbourhood group
pivot_table = data.pivot_table(values='price', index='room_type', columns='neighbourhood_group', aggfunc='mean')

# Create the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(pivot_table, annot=True, cmap="coolwarm", fmt=".2f", cbar_kws={'label': 'Average Price ($)'})
plt.title("Heatmap of Average Price by Room Type and Borough")
plt.ylabel("Room Type")
plt.xlabel("Borough")
plt.tight_layout()
plt.show()

room_counts = data.groupby(['neighbourhood_group', 'room_type']).size().reset_index(name='count')

most_used_room_type = room_counts.loc[room_counts.groupby('neighbourhood_group')['count'].idxmax()]

print(most_used_room_type)

room_counts = data.groupby(['neighbourhood_group', 'room_type']).size().reset_index(name='count')
plt.figure(figsize=(12, 6))
room_counts = room_counts.sort_values(by='count', ascending=False)
sns.barplot(data=room_counts, x='neighbourhood_group', y='count', hue='room_type', palette='viridis')
plt.title("Prefered room type in neighbourhood_group")
plt.xlabel("Neighbourhood Group")
plt.ylabel("Counts")
plt.show()

plt.figure(figsize=(10,10))
ax = sns.boxplot(data=data, x='neighbourhood_group',y='availability_365')

# Columns to analyze
columns_to_analyze = ['minimum_nights']

def analyze_and_visualize_column(df, column):
    # Calculate statistics
    mean_val = df[column].mean()
    median_val = df[column].median()
    min_val = df[column].min()
    max_val = df[column].max()

    # Print results
    print(f"Analysis of '{column}':")
    print(f"  Mean: {mean_val:.2f}")
    print(f"  Median: {median_val:.2f}")
    print(f"  Range: {max_val - min_val:.2f} (Min: {min_val:.2f}, Max: {max_val:.2f})")
    print("\n")

    # Create pie chart
    plt.figure(figsize=(10, 6))
    value_counts = df[column].value_counts().sort_index().head(5)
    plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')
    plt.title(f'Distribution of {column} (Top 5 values)')
    plt.axis('equal')
    plt.show()

# Analyze and visualize each column
for column in columns_to_analyze:
    analyze_and_visualize_column(data, column)

# Scatterplot: Price vs Latitude and Longitude with color by Neighbourhood Group
plt.figure(figsize=(12, 8))
scatter = sns.scatterplot(
    x=data['longitude'],
    y=data['latitude'],
    hue=data['neighbourhood_group'],  # Color by Neighbourhood Group
    size=data['price'],  # Optional: size points by price
    sizes=(10, 100),  # Minimum and maximum point sizes
    alpha=0.7,
    palette='Set1'  # Choose a color palette
)

# Plot labels and title
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Scatterplot of Price by Location (Latitude vs Longitude)')
plt.legend(title='Neighbourhood Group', loc='upper right')
plt.grid(alpha=0.3)

# Display the plot
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split # Import the necessary function
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


data = data.dropna(subset=['price', 'reviews_per_month'])

# Define features (independent variables) and target (dependent variable)
features = ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'availability_365']
X = data[features]
y = data['price']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)

# Display coefficients
coefficients = pd.DataFrame({
    'Feature': features,
    'Coefficient': model.coef_
})
print(coefficients)

import sqlite3

# Define the SQLite database path
database_path = 'ab_nyc_data.sqlite'

# Connect to an SQLite database
conn = sqlite3.connect(database_path)

# Export DataFrame to SQLite database
data.to_sql('ab_nyc_listings', conn, if_exists='replace', index=False)

# Close the connection to the database
conn.close()
print(f"Data exported to SQLite database at: {database_path}")

# Set Current Working Directory
current_directory = os.getcwd()
print(f"Current working directory: {current_directory}")

# Reconnect to the SQLite database to query data
conn = sqlite3.connect(database_path)

# Query the data retrieved from the database using read_sql_query
df_from_db = pd.read_sql_query('SELECT * FROM ab_nyc_listings', conn)

# Close the connection
conn.close()

# Show the data output after retrieval to test functionality of the codes above
print("Data extracted from AB NYC SQLite database:")
print(df_from_db.head())

## This completes the Part V

## Thank you for Reading the codes

## Group5: Team Members : Krishi Shah, Devansh Bhavsar, Nisargvan Goswami, Dhruv Sharma,Devarshi Lala